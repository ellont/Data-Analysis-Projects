{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "import zipfile\n",
    "# from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import openpyxl\n",
    "from os.path import basename\n",
    "from datetime import time,timedelta\n",
    "from nptime import nptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(time_string):\n",
    "    time_split = time_string.split(':')\n",
    "    hour = int(time_split[0])\n",
    "    if hour >= 24:\n",
    "        hour -= 24\n",
    "        time_split[0] = '%.2d'% (hour)\n",
    "    else:\n",
    "        hour = '%.2d'% (hour) \n",
    "    hour_var = ':'.join(time_split)\n",
    "\n",
    "    return hour_var\n",
    "\n",
    "# intervals needs to be integer that can be divided by 1440 (minutes in day) with remainder as 0\n",
    "def time_to_intervals_string(time_string,interval_in_minutes):\n",
    "    total_minutes_in_a_day = 24 * 60\n",
    "\n",
    "    if total_minutes_in_a_day%interval_in_minutes != 0:\n",
    "        print('Invalid Intervals!')\n",
    "    else:\n",
    "        time_split = time_string.split(':')\n",
    "        hour = int(time_split[0])\n",
    "        minute = int(time_split[1])\n",
    "        second = int(time_split[2])\n",
    "\n",
    "        interval =  (hour*60 + minute) // interval_in_minutes\n",
    "\n",
    "        start_total_minutes = interval_in_minutes * interval\n",
    "        # end_total_minutes = interval_in_minutes * (interval + 1)\n",
    "\n",
    "        start_hour = start_total_minutes // 60\n",
    "        start_minute = start_total_minutes % 60\n",
    "        start_time = nptime(start_hour,start_minute,second)\n",
    "        end_time = start_time + timedelta(minutes=interval_in_minutes)\n",
    "        interval_string = str(start_time) # +' - ' + str(end_time)\n",
    "        # end_hour = end_total_minutes // 60\n",
    "        # end_minute = end_total_minutes % 60\n",
    "\n",
    "        # interval_string = '%.2d:%.2d:00 - %.2d:%.2d:00'% (start_hour,start_minute,end_hour,end_minute)\n",
    "\n",
    "    return interval_string\n",
    "\n",
    "def dateRange_to_dates(raw_calendar):\n",
    "    num_service_id = raw_calendar.shape[0] # number of service_ids\n",
    "    list = []\n",
    "    i = 0\n",
    "    while i < num_service_id:\n",
    "        start_date = str(raw_calendar.iloc[i,-2])\n",
    "        end_date = str(raw_calendar.iloc[i,-1])\n",
    "        dates = pd.date_range(start_date,end_date)\n",
    "        i += 1\n",
    "        list.append(dates)\n",
    "\n",
    "    return list\n",
    "\n",
    "def load_file(file_folder_location, file_name):\n",
    "    with open(os.path.join(file_folder_location,file_name), 'rb') as f:\n",
    "        zip_file = zipfile.ZipFile(BytesIO(f.read()))\n",
    "        \n",
    "    dfst = {}\n",
    "    for name in zip_file.namelist():\n",
    "        dfst[name.lower()[:-4]] = pd.read_csv(zip_file.open(name))\n",
    "\n",
    "    return dfst\n",
    "\n",
    "def download_zipfile(file, file_folder_location, file_name):\n",
    "        with open(os.path.join(file_folder_location,file_name), 'wb') as f:\n",
    "    # Set the chunk size\n",
    "            chunk_size = 4096\n",
    "\n",
    "  # Read the file in chunks\n",
    "            while True:\n",
    "    # Read a chunk\n",
    "                chunk = file.read(chunk_size)\n",
    "\n",
    "    # Break if the chunk is empty\n",
    "                if not chunk:\n",
    "                    break\n",
    "\n",
    "    # Write the chunk to the file\n",
    "                f.write(chunk)\n",
    "\n",
    "def save_csvFile(file_folder_location, file_name, df):\n",
    "    file_path = os.path.join(file_folder_location,file_name)\n",
    "    df.to_csv(file_path, index = False, chunksize=1000)\n",
    "\n",
    "def delete_file (file_folder_location, file_name):\n",
    "    file_path = os.path.join(file_folder_location,file_name)\n",
    "    try:\n",
    "    # Try to delete the file\n",
    "        os.remove(file_path)\n",
    "    except FileNotFoundError:\n",
    "    # Do nothing if the file doesn't exist\n",
    "        pass\n",
    "\n",
    "def id_to_string(dfst):\n",
    "    dfst['stop_times']['stop_id'] = dfst['stop_times']['stop_id'].astype('string')\n",
    "    dfst['stop_times']['trip_id'] = dfst['stop_times']['trip_id'].astype('string')\n",
    "    dfst['stops']['stop_id'] = dfst['stops']['stop_id'].astype('string')\n",
    "    dfst['trips']['trip_id'] = dfst['trips']['trip_id'].astype('string')\n",
    "    dfst['trips']['service_id'] = dfst['trips']['service_id'].astype('string')\n",
    "    dfst['trips']['route_id'] = dfst['trips']['route_id'].astype('string')\n",
    "    dfst['calendar']['service_id'] = dfst['calendar']['service_id'].astype('string')\n",
    "    dfst['routes']['route_id'] = dfst['routes']['route_id'].astype('string')\n",
    "    dfst['routes']['agency_id'] = dfst['routes']['agency_id'].astype('string')\n",
    "    dfst['stop_times']['departure_time'] = dfst['stop_times']['departure_time'].astype('string')\n",
    "\n",
    "def new_calendar(calendar_dfst):\n",
    "    date_list = dateRange_to_dates(calendar_dfst)\n",
    "    service_id = calendar_dfst['service_id'].tolist()\n",
    "    cal = pd.DataFrame({'service_id': service_id, 'date':date_list}).explode('date')\n",
    "    #add date column, explode by date\n",
    "    calendar_file = calendar_dfst.merge(cal) \n",
    "\n",
    "    #convert 1 to the respective column name (day name)\n",
    "    for col in calendar_file.columns[1:8]:\n",
    "        calendar_file[col] = calendar_file[col].apply(lambda x: col if x == 1 else 0)\n",
    "\n",
    "    calendar_file['day'] = calendar_file['date'].apply(lambda x: x.strftime(\"%A\").lower())\n",
    "    calendar_file = calendar_file.loc[calendar_file.iloc[:,1:8].apply(lambda x: x == calendar_file['day']).any(1)]\n",
    "\n",
    "\n",
    "    # calendar_file = calendar_file.iloc[:,[0,-2]] # trim calendar file\n",
    "    calendar_file = calendar_file.loc[:,['service_id','date']]\n",
    "    \n",
    "    #merge with calendar_dates file\n",
    "\n",
    "    return calendar_file\n",
    "\n",
    "def filtered_calendar (calendar_dfst, calendar_dates_dfst):\n",
    "    calendar_file = new_calendar(calendar_dfst)\n",
    "\n",
    "    #convert date type to datetime\n",
    "    calendar_dates_dfst['date'] = pd.to_datetime(calendar_dates_dfst['date'],format='%Y%m%d')\n",
    "\n",
    "    #merge calendar_file with calendar_dates file\n",
    "    calendar_file = pd.merge(calendar_file, calendar_dates_dfst, how='outer', left_on=['service_id','date'], right_on=['service_id','date'])\n",
    "\n",
    "    #filter out the service dates that are removed (exception type = 2)\n",
    "    calendar_file = calendar_file.loc[calendar_file['exception_type'] != 2]\n",
    "    calendar_file = calendar_file.loc[:,['service_id','date']]\n",
    "        \n",
    "    return calendar_file\n",
    "\n",
    "def dedup_calendar(start_date_from_last_file,calendar_dfst):\n",
    "    #filter out the servcie dates by start date from the previous file to remove duplicates    \n",
    "    if start_date_from_last_file != '':\n",
    "        calendar_dfst = calendar_dfst[calendar_dfst['date'] < start_date_from_last_file]      \n",
    "\n",
    "    return calendar_dfst\n",
    "\n",
    "def routes_by_agencyId (route_dfst, agencyIds_list): #filter routes table by agency id\n",
    "    frames = []\n",
    "    for agency_id in agencyIds_list:\n",
    "        frames.append(route_dfst[route_dfst['agency_id'] == agency_id])\n",
    "    #routes = route_dfst[route_dfst['agency_id'] == agencyId]\n",
    "    # train agency id 1 & 2\n",
    "    routes = pd.concat(frames,ignore_index=True)\n",
    "    \n",
    "    return routes\n",
    "\n",
    "def add_to_zipfile(file_folder_location, zip_name, csv_name):\n",
    "    filepath = os.path.join(file_folder_location, zip_name)\n",
    "    #append to the existing zip file.\n",
    "    #create a new zip file if doesn't exsit already\n",
    "    #zipfile.ZIP_DEFLATED <- to compress the file\n",
    "    with zipfile.ZipFile(filepath, 'a', zipfile.ZIP_DEFLATED) as z:\n",
    "        path = os.path.join(file_folder_location, csv_name)\n",
    "        z.write(path, basename(path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get stop_times information by dates, agency_id\n",
    "def get_merged_file(dfst, calendar_file, agencyIds_list):\n",
    "\n",
    "    routes = routes_by_agencyId(route_dfst=dfst['routes'], agencyIds_list=\n",
    "    agencyIds_list)\n",
    "    #train_route_ids = train_route['route_id']\n",
    "\n",
    "    # Get trip_ids by filtering Trips table using route_id & service_id\n",
    "    trips = dfst['trips'].loc[dfst['trips']['route_id'].isin(routes['route_id'])]\n",
    "    trips = trips.loc[trips['service_id'].isin(calendar_file['service_id'])]\n",
    "    trips = trips.merge(routes, how = 'left', on = 'route_id')\n",
    "\n",
    "    # get the final stop_times by filtering stop_time table using trip_id\n",
    "    stop_times = dfst['stop_times'].loc[dfst['stop_times']['trip_id'].isin(trips['trip_id'])]\n",
    "    # merge with stops table to get stop name\n",
    "    stop_times = stop_times.merge(dfst['stops'], how = 'left', on = 'stop_id')\n",
    "    stop_times = stop_times.loc[:,['trip_id','stop_id','stop_name','departure_time']]\n",
    "\n",
    "    #final merge\n",
    "    #merge trip table with stop_times table\n",
    "    final_merge = stop_times.merge(trips, how = 'left', on = 'trip_id')\n",
    "    final_merge = final_merge.loc[:,['stop_id','stop_name','service_id','trip_id','departure_time','trip_headsign','agency_id']]\n",
    "\n",
    "    final_merge = final_merge.merge(calendar_file, how = 'left', on = 'service_id')\n",
    "    \n",
    "    return final_merge\n",
    "\n",
    "def datetime_formatting(merged_file):\n",
    "    #add column 'next_day' if departure_time is after 23:59:00, assign 1 as true, 0 as false\n",
    "    merged_file['next_day'] = merged_file['departure_time'].apply(lambda x: 1 if int(x.split(':')[0]) > 23 else 0)\n",
    "\n",
    "    #add column 'service_date'. Its value is date + next_day (i.e. if date is 2022/01/01 and the departure_time is after 24:10:00, next_day is 1 and service_date is 2022/01/02)\n",
    "    merged_file['service_date'] = merged_file['date'] + pd.to_timedelta(merged_file['next_day'],unit='D')\n",
    "\n",
    "    #convert departure_time to 24HR format\n",
    "    merged_file['departure_time'] = merged_file['departure_time'].apply(lambda x: convert_time(x))\n",
    "\n",
    "def add_intervals(merged_file, interval):\n",
    "    #add intervals based on departure_time.\n",
    "    merged_file['interval'] = merged_file['departure_time'].apply(lambda x: time_to_intervals_string(x, interval))\n",
    "\n",
    "def add_direction(merged_file):\n",
    "    #add direction column to the file\n",
    "    #into city is 1 | out of city is 0\n",
    "    city = re.compile('.*City')\n",
    "    to_city_headsign = list(filter(city.match, merged_file['trip_headsign'].unique()))\n",
    "    to_city_headsign.extend(['Melbourne','Flinders Street','Southern Cross'])\n",
    "    merged_file['direction'] = merged_file['trip_headsign'].apply(lambda x: 1 if x in(to_city_headsign) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17584\\3303725624.py:119: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  calendar_file = calendar_file.loc[calendar_file.iloc[:,1:8].apply(lambda x: x == calendar_file['day']).any(1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar - Done!\n",
      "Merge done!\n",
      "Result file done!\n",
      "Result saved!\n",
      "20200110 - Completed\n",
      "Page 15 is completed\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "page_number = 15\n",
    "page_link = 'https://transitfeeds.com/p/ptv/497?p='\n",
    "folder_location = 'C:/Users/Administrator/Desktop/gtfs/'\n",
    "head = 'https://transitfeeds.com'\n",
    "start_date = ''\n",
    "\n",
    "# agency_id for train\n",
    "Vline = '1'\n",
    "Metro = '2'\n",
    "ids = [Vline, Metro]\n",
    "\n",
    "interval_mins = 30\n",
    "\n",
    "page = 1\n",
    "while page <= page_number:\n",
    "    url = page_link + str(page)\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text,'html.parser')\n",
    "\n",
    "\n",
    "    for link in soup.find_all('a', text='Download'):\n",
    "        #download the zipfile into the directory\n",
    "        name = str(link).split('/')\n",
    "        name = name[4]\n",
    "        zipFile_name = name +'.zip'\n",
    "        result_filename = name + '.csv'\n",
    "\n",
    "        print(zipFile_name)\n",
    "        link = head + link.get('href')\n",
    "        print(link)\n",
    "        zipFile = urllib.urlopen(link)\n",
    "        download_zipfile(zipFile, folder_location, zipFile_name)\n",
    "        \n",
    "        #load files\n",
    "        dfst = load_file(folder_location,zipFile_name)    \n",
    "\n",
    "        #setup files\n",
    "        id_to_string(dfst)\n",
    "\n",
    "        #setup calendar file   \n",
    "        calendar = filtered_calendar(calendar_dfst=dfst['calendar'], calendar_dates_dfst=dfst['calendar_dates'])\n",
    "        start = calendar['date'].min()\n",
    "        calendar = dedup_calendar(start_date_from_last_file=start_date,calendar_dfst=calendar)\n",
    "        start_date = start\n",
    "        print('Calendar - Done!')\n",
    "\n",
    "        final_merge = get_merged_file(dfst=dfst,calendar_file=calendar,agencyIds_list=ids)\n",
    "        datetime_formatting(final_merge)\n",
    "        add_intervals(final_merge,interval_mins)\n",
    "        add_direction(final_merge)\n",
    "        print('Merge done!')\n",
    "\n",
    "        #save mergedfile just in case program crashes\n",
    "        # merged_filename = name + '_merged.csv'\n",
    "        # save_csvFile(file_folder_location=folder_location,file_name=merged_filename,df=final_merge)\n",
    "\n",
    "        #get the count number of services by stop, date, interval, directiobn\n",
    "        result = final_merge.groupby(['stop_id','stop_name','date','interval','trip_headsign','direction','agency_id'])['interval'].size().to_frame(name = 'count').reset_index()\n",
    "        print('Result file done!')\n",
    "        #save the result as csv\n",
    "        save_csvFile(file_folder_location=folder_location,file_name=result_filename,df=result)\n",
    "        add_to_zipfile(file_folder_location=folder_location, zip_name='result.zip', csv_name=result_filename)\n",
    "        print('Result saved!')\n",
    "\n",
    "\n",
    "        print(name + ' - Completed')\n",
    "        delete_file(file_folder_location=folder_location, file_name=zipFile_name)\n",
    "        # delete_file(file_folder_location=folder_location,file_name=merged_filename)\n",
    "        gc.collect()\n",
    "    \n",
    "\n",
    "    print('Page %d is completed'% (page))\n",
    "    page += 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat all individual files into one\n",
    "\n",
    "file_folder_location = 'C:/Users/Administrator/Desktop/gtfs/'\n",
    "file_name = 'results.zip'\n",
    "\n",
    "with open(os.path.join(file_folder_location,file_name), 'rb') as f:\n",
    "    zip_file = zipfile.ZipFile(BytesIO(f.read()))\n",
    "    \n",
    "dfst = {}\n",
    "frames = []\n",
    "for name in zip_file.namelist():\n",
    "    df_name = name.lower()[:-4]\n",
    "    dfst[df_name] = pd.read_csv(zip_file.open(name))\n",
    "    frames.append(dfst[df_name])\n",
    "\n",
    "final_data = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "save_csvFile(file_folder_location=file_folder_location,file_name='final_result.csv',df=final_data)\n",
    "add_to_zipfile(file_folder_location=file_folder_location, zip_name='final_result.zip', csv_name='final_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = final_data.loc[final_data['trip_headsign'] == 'Flinders Street Southern Cross Station']\n",
    "test = final_data.loc[final_data['agency_id'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sunbury', 'City (Flinders Street)', 'Seymour', 'Melbourne',\n",
       "       'Shepparton', 'Frankston', 'Stony Point', 'Glen Waverley',\n",
       "       'Hurstbridge', 'Sandringham', 'Belgrave', 'Alamein', 'Lilydale',\n",
       "       'Craigieburn', 'Mernda', 'Pakenham', 'Werribee', 'Upfield',\n",
       "       'Williamstown', 'Flinders Street', 'Cranbourne', 'Ballarat',\n",
       "       'Ararat', 'Geelong', 'Bendigo', 'Echuca/Moama', 'Albury',\n",
       "       'Warrnambool', 'Swan Hill', 'Nhill', 'Maryborough',\n",
       "       'Showgrounds/Flemington', 'Traralgon', 'Bairnsdale', 'City',\n",
       "       'Clockwise', 'Richmond (MCG)', 'City (Southern Cross)',\n",
       "       'South Morang/Mernda', 'South Morang', 'Marshall',\n",
       "       'Melbourne Broadmeadows', 'Melbourne Seymour',\n",
       "       'Seymour Broadmeadows', 'Shepparton Seymour',\n",
       "       'Flinders Street Southern Cross Station', 'Melbourne Melton',\n",
       "       'Ballarat Melton', 'Ararat Ballarat', 'Melbourne Ballarat',\n",
       "       'Melbourne Sunbury', 'Bendigo Sunbury',\n",
       "       'Melbourne Bendigo or Heathcote',\n",
       "       'Echuca/Moama Bendigo or Heathcote', 'Albury Seymour',\n",
       "       'Bairnsdale Traralgon & Sale', 'Warrnambool Geelong & Colac',\n",
       "       'Swan Hill Bendigo', 'Melbourne Morwell, Moe & Pakenham',\n",
       "       'Melbourne Colac & Geelong', 'Melbourne Sale & Traralgon',\n",
       "       'Melbourne Bendigo', 'Traralgon Pakenham, Moe & Morwell',\n",
       "       'Maryborough Ballarat',\n",
       "       'Showgrounds/Flemington Southern Cross Station',\n",
       "       'Richmond (MCG) Geelong', 'Marshall Geelong'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_headsign = final_data['trip_headsign'].unique()\n",
    "uni_headsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sunbury', 'City (Flinders Street)', 'Frankston', 'Stony Point',\n",
       "       'Glen Waverley', 'Hurstbridge', 'Sandringham', 'Belgrave',\n",
       "       'Alamein', 'Lilydale', 'Craigieburn', 'Mernda', 'Pakenham',\n",
       "       'Werribee', 'Upfield', 'Williamstown', 'Flinders Street',\n",
       "       'Cranbourne', 'Showgrounds/Flemington', 'Clockwise',\n",
       "       'South Morang/Mernda', 'South Morang',\n",
       "       'Flinders Street Southern Cross Station',\n",
       "       'Showgrounds/Flemington Southern Cross Station'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['trip_headsign'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_30m(time_string): \n",
    "    time_split = time_string.split(' - ')\n",
    "    start_time = time_split[0]\n",
    "    start = start_time.split(':')\n",
    "    start_hr = int(start[0])\n",
    "    start_min = int(start[1])\n",
    "\n",
    "    end_time = time_split[1]\n",
    "    end = end_time.split(':')    \n",
    "    end_hr = int(end[0])\n",
    "    end_min = int(end[1])\n",
    "\n",
    "    #00:00 - 29:59 ; 30:00 - 59:59\n",
    "    if start_min < 30:\n",
    "        start_min = 0\n",
    "        end_min = 29\n",
    "    else:\n",
    "        start_min = 30\n",
    "        end_min = 59\n",
    "\n",
    "    new_interval = '%.2d:%.2d:00 - %.2d:%.2d:59'% (start_hr,start_min,end_hr,end_min)\n",
    "\n",
    "    return new_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_location = 'C:/Users/Administrator/Desktop/gtfs/'\n",
    "file_name = 'final_result.zip'\n",
    "result_filename = 'new_result.csv'\n",
    "\n",
    "#dfst = load_file(file_folder_location,file_name)\n",
    "with open(os.path.join(folder_location,file_name), 'rb') as f:\n",
    "    zip_file = zipfile.ZipFile(BytesIO(f.read()))\n",
    "    \n",
    "dfst = pd.read_csv(zip_file.open('final_result.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfst['30m_interval'] = dfst['interval'].apply(lambda x: interval_30m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dfst.groupby(['stop_id','stop_name','date','30m_interval','direction'])['count'].sum().to_frame(name = '30m_count').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csvFile(file_folder_location=folder_location,file_name=result_filename,df=result)\n",
    "add_to_zipfile(file_folder_location=folder_location, zip_name='new_result.zip', csv_name=result_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = result.loc[:,['stop_id','stop_name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stops.groupby(['stop_id','stop_name']).size().to_frame(name = 'count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csvFile(file_folder_location=folder_location,file_name='stops.csv',df=stops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
